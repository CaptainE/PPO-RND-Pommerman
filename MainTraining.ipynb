{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pommerman - training\n",
    "## By: Peter Ebert Christensen (s153758), Johan Bloch Madsen(s152991) and Mads Okholm BjÃ¸rn (s153413)\n",
    "\n",
    "Pommerman can be installed by following the instructions at the following link: https://github.com/MultiAgentLearning/playground\n",
    "\n",
    "Docker can be installed by following the instructions at the following link:\n",
    "https://docs.docker.com/install/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will print:\n",
    "# 'Import error NSDE! You will not be able to render --> Cannot connect to \"None\"' \n",
    "# if run on headless server, no need to worry\n",
    "import pommerman\n",
    "from pommerman import agents\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import time\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "# Our own files\n",
    "from convertInputMapToTrainingLayers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the networks\n",
    "\n",
    "First our main network, an ActorCritic network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.critic_con = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=7,\n",
    "                      out_channels=64, \n",
    "                      kernel_size=3, \n",
    "                      padding=0),\n",
    "            nn.Conv2d(in_channels=64,\n",
    "                      out_channels=64, \n",
    "                      kernel_size=3, \n",
    "                      padding=0),\n",
    "            nn.Conv2d(in_channels=64,\n",
    "                      out_channels=64, \n",
    "                      kernel_size=3, \n",
    "                      padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.critic_linear = nn.Sequential(\n",
    "            nn.Linear(3*3*64, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.actor_con = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=7,\n",
    "                      out_channels=64, \n",
    "                      kernel_size=3, \n",
    "                      padding=0),\n",
    "            nn.Conv2d(in_channels=64,\n",
    "                      out_channels=64, \n",
    "                      kernel_size=3, \n",
    "                      padding=0),\n",
    "            nn.Conv2d(in_channels=64,\n",
    "                      out_channels=64, \n",
    "                      kernel_size=3, \n",
    "                      padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor_linear = nn.Sequential(\n",
    "            nn.Linear(3*3*64, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_outputs)\n",
    "        )\n",
    "        \n",
    "        self.log_std = nn.Parameter(torch.ones(num_outputs) * std)\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value = self.critic_con(x)\n",
    "        value = self.critic_linear(value.view(-1, 3*3*64))\n",
    "        \n",
    "        mu    = self.actor_con(x)\n",
    "        mu    = self.actor_linear(mu.view(-1, 3*3*64))\n",
    "        \n",
    "        std1  = self.log_std.exp()\n",
    "        std   = std1.expand_as(mu)\n",
    "        dist  = Normal(mu, std)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then our random network for random network distillation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RND(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size):\n",
    "        super(RND, self).__init__()\n",
    "        self.Feature = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=7,\n",
    "                      out_channels=64, \n",
    "                      kernel_size=3, \n",
    "                      padding=0),\n",
    "            nn.Conv2d(in_channels=64,\n",
    "                      out_channels=64, \n",
    "                      kernel_size=3, \n",
    "                      padding=0),\n",
    "            nn.Conv2d(in_channels=64,\n",
    "                      out_channels=64, \n",
    "                      kernel_size=3, \n",
    "                      padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.Feature_linear = nn.Sequential(\n",
    "            nn.Linear(3*3*64, hidden_size),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        value = self.Feature(x)\n",
    "        value = self.Feature_linear(value.view(-1, 3*3*64))\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs       = 324\n",
    "num_outputs      = 6\n",
    "hidden_size      = 1024\n",
    "lr               = 1e-6\n",
    "lr_RND           = 1e-3\n",
    "mini_batch_size  = 5\n",
    "ppo_epochs       = 4\n",
    "max_frames       = 1500000\n",
    "frame_idx        = 0\n",
    "game_idx         = 0\n",
    "device           = \"cpu\" # Hard-coded since we have a GPU, but does not want to use\n",
    "clip_param       = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Networks\n",
    "\n",
    "We normaly need to train about 3 000 000 frames to get a win rate above 50 % against 3 simple agents, and train about 500 000 to see an improvement from the random weights.\n",
    "The win rate is based on test play against 3 simple agents in FFA mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ActorCritic(num_inputs, num_outputs, hidden_size).to(device)\n",
    "RandomNN = RND(num_inputs, num_outputs, hidden_size).to(device)\n",
    "PredictorNN = RND(num_inputs, num_outputs, hidden_size).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "predictorOptim = optim.Adam(PredictorNN.parameters(), lr=lr_RND)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "\n",
    "test_rewards     = []\n",
    "d3s              = [[], [], []]\n",
    "existingAggregate =(0,0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO update functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a new value newValue, compute the new count, new mean, the new M2.\n",
    "# mean accumulates the mean of the entire dataset\n",
    "# M2 aggregates the squared distance from the mean\n",
    "# count aggregates the number of samples seen so far\n",
    "def varupdate(existingAggregate, newValue):\n",
    "    (count, mean, M2) = existingAggregate\n",
    "    count += 1 \n",
    "    delta = newValue - mean\n",
    "    mean += delta / count\n",
    "    delta2 = newValue - mean\n",
    "    M2 += delta * delta2\n",
    "\n",
    "    return (count, mean, M2)\n",
    "\n",
    "# retrieve the mean, variance and sample variance from an aggregate\n",
    "def varfinalize(existingAggregate):\n",
    "    (count, mean, M2) = existingAggregate\n",
    "    (mean, variance, sampleVariance) = (mean, M2/count, M2/(count - 1)) \n",
    "    if count < 2:\n",
    "        return float('nan')\n",
    "    else:\n",
    "        return (mean, variance, sampleVariance)\n",
    "    \n",
    "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
    "    batch_size = states.size(0)\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]\n",
    "        \n",
    "        \n",
    "\n",
    "def ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages):\n",
    "    for _ in range(ppo_epochs):\n",
    "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
    "            dist, value = model(state)\n",
    "            entropy = dist.entropy().mean()\n",
    "            new_log_probs = dist.log_prob(action)\n",
    "\n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
    "\n",
    "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (return_ - value).pow(2).mean()\n",
    "\n",
    "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def compute_gae(next_value, rewards, masks, values, gamma=0.8, tau=0.9):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of agents (exactly four)\n",
    "agent_list = [\n",
    "    agents.RandomAgent(), # Does not matter, we control this agent\n",
    "    agents.SimpleAgent(),\n",
    "    agents.SimpleAgent(),\n",
    "    agents.SimpleAgent(),\n",
    "    # agents.DockerAgent(\"pommerman/simple-agent\", port=12345),\n",
    "]\n",
    "# Make the \"Free-For-All\" environment using the agent list\n",
    "env = pommerman.make('PommeFFACompetitionFast-v0', agent_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup testing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_env(vis=False):\n",
    "    state = env.reset()\n",
    "    if vis: env.render()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    ties = 0\n",
    "    while not done:\n",
    "        stateOrginal = state\n",
    "        state = torch.FloatTensor(stateToTorch(state)).to(device)\n",
    "        dist, _ = model(state)\n",
    "        actionsList = env.act(stateOrginal)\n",
    "        next_state, reward, done, info = env.step([dist.mean.cpu().data.numpy()[0].argmax()] + actionsList[1:])\n",
    "        state = next_state\n",
    "        #show_state(env, stateOrginal, step=0, info=\"info\")\n",
    "        if vis: env.render()\n",
    "    if \"winners\" in info:\n",
    "        if 0 in info[\"winners\"]:\n",
    "            #print(\"Winner!\", info)\n",
    "            total_reward += 0\n",
    "            wins += 1\n",
    "        else:\n",
    "            #print(\"Loser!\", info)\n",
    "            total_reward += -1\n",
    "            losses += 1\n",
    "    else:\n",
    "        #print(\"Tie!\", info)\n",
    "        total_reward += 0\n",
    "        ties += 1\n",
    "    return total_reward, wins, losses, ties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "In the below code we train for 100 games (each game is a training batch), then we test on 10 games, then repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are we alive: False, Game ended after 76 rounds, total frames played: 75\n",
      "Are we alive: False, Game ended after 10 rounds, total frames played: 84\n",
      "Are we alive: False, Game ended after 10 rounds, total frames played: 93\n",
      "Are we alive: False, Game ended after 10 rounds, total frames played: 102\n",
      "Are we alive: False, Game ended after 232 rounds, total frames played: 333\n",
      "Are we alive: False, Game ended after 337 rounds, total frames played: 669\n",
      "Are we alive: False, Game ended after 117 rounds, total frames played: 785\n",
      "Are we alive: False, Game ended after 522 rounds, total frames played: 1306\n",
      "Are we alive: False, Game ended after 84 rounds, total frames played: 1389\n",
      "Are we alive: False, Game ended after 260 rounds, total frames played: 1648\n",
      "Are we alive: False, Game ended after 439 rounds, total frames played: 2086\n"
     ]
    }
   ],
   "source": [
    "while frame_idx < max_frames:\n",
    "    log_probs = []\n",
    "    values    = []\n",
    "    states    = []\n",
    "    actions   = []\n",
    "    rewards   = []\n",
    "    RNDrewards = []\n",
    "    masks     = []\n",
    "    entropy = 0\n",
    "\n",
    "\n",
    "    done = False\n",
    "    gameSteps = 0\n",
    "    info = \"\"\n",
    "    dead = False\n",
    "\n",
    "    state = env.reset()\n",
    "    game_idx += 1\n",
    "\n",
    "    actionDist = [0, 0, 0, 0, 0, 0]\n",
    "    actionDistLogic = [0, 0, 0, 0, 0, 0]\n",
    "    actionsTaken = []\n",
    "    avgP=0\n",
    "    deadLastRound = 0\n",
    "    while not done:\n",
    "        gameSteps += 1\n",
    "        stateOrginal = state\n",
    "        state = stateToTorch(state)\n",
    "\n",
    "        ai = [\n",
    "            createLayerWalkable(stateOrginal[0]),\n",
    "            createLayerBoxes(stateOrginal[0]),\n",
    "            createLayerDangerMap(stateOrginal[0]),\n",
    "            createLayerFriendsAndEnemies(stateOrginal[0])\n",
    "        ]\n",
    "        d = []\n",
    "        if stateOrginal[0][\"ammo\"]==0:\n",
    "            d.append(10)\n",
    "        else:    \n",
    "            d.append(ai[2][4][4])\n",
    "        d.append((1 - ai[0][3][4]) + ai[2][3][4])\n",
    "        d.append((1 - ai[0][5][4]) + ai[2][5][4])\n",
    "        d.append((1 - ai[0][4][3]) + ai[2][4][3])\n",
    "        d.append((1 - ai[0][4][5]) + ai[2][4][5])\n",
    "        sumDanger = 0\n",
    "        for x in range(3, 6):\n",
    "            for y in range(3, 6):\n",
    "                sumDanger += ai[2][x][y]\n",
    "        if stateOrginal[0][\"ammo\"]!=0 and sumDanger==0:\n",
    "            d.append(0)\n",
    "        elif stateOrginal[0][\"ammo\"]==0:\n",
    "            d.append(10)\n",
    "        else:\n",
    "            d.append(sumDanger)\n",
    "        d = [-dd for dd in d]\n",
    "        actionLogic = d.index(max(d))\n",
    "        actionDistLogic[actionLogic] += 1\n",
    "\n",
    "        dist, value = model(state)\n",
    "\n",
    "        action = dist.sample()\n",
    "        actionsList = env.act(stateOrginal)\n",
    "        actionNetwork = action.argmax().data[0].cpu().numpy();\n",
    "        actionsTaken.append(actionNetwork)\n",
    "        actionDist[actionNetwork] += 1\n",
    "        next_state, reward, done, info = env.step([actionNetwork] + actionsList[1:])\n",
    "        rewardGivenbyUs=np.asscalar(-loss(action.data[0],torch.FloatTensor(d)).data[0].cpu().numpy())\n",
    "        target=RandomNN(stateToTorch(next_state))\n",
    "        predict=PredictorNN(stateToTorch(next_state))\n",
    "        v=loss(target,predict)\n",
    "        avgP+=v.data[0].cpu().numpy()\n",
    "        if existingAggregate[2]>1:\n",
    "            var =varfinalize(existingAggregate)\n",
    "            rewards.append((v/math.sqrt(var[1]))+rewardGivenbyUs+reward[0]*100)\n",
    "        else:\n",
    "            rewards.append(v+rewardGivenbyUs+reward[0]*100)\n",
    "        RNDrewards.append(v)    \n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy += dist.entropy().mean()\n",
    "\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "\n",
    "\n",
    "\n",
    "        masks.append(torch.FloatTensor([1 - deadLastRound]).unsqueeze(1).to(device))\n",
    "\n",
    "        deadLastRound = reward[0] == -1\n",
    "        if (deadLastRound):\n",
    "            dead = True\n",
    "            break\n",
    "\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "\n",
    "        state = next_state\n",
    "        frame_idx += 1\n",
    "\n",
    "\n",
    "    avgR = 0\n",
    "    countR = 0\n",
    "    for r in rewards:\n",
    "        avgR += r\n",
    "    #print(game_idx, \"gameLength: \", gameSteps, dead, float(avgR)/gameSteps,float(avgP)/gameSteps, actionDist)\n",
    "\n",
    "    if game_idx % 100 == 0:\n",
    "        test_rewardsList = []\n",
    "        winsList = []\n",
    "        lossesList = []\n",
    "        tiesList = []\n",
    "        for _ in range(10):\n",
    "            test_reward, wins, losses, ties = test_env()\n",
    "            test_rewardsList.append(test_reward)\n",
    "            winsList.append(wins)\n",
    "            lossesList.append(losses)\n",
    "            tiesList.append(ties)\n",
    "        test_reward = np.mean(test_rewardsList)\n",
    "        test_rewards.append(test_reward)\n",
    "\n",
    "        wins = np.mean(winsList)\n",
    "        losses = np.mean(lossesList)\n",
    "        ties = np.mean(tiesList)\n",
    "\n",
    "        d3s[0].append(wins)\n",
    "        d3s[1].append(losses)\n",
    "        d3s[2].append(ties)\n",
    "        print(\"Out of 10 test games, we won: \" + str(sum(winsList)) + \", we tied: \" + str(sum(tiesList)) + \", we lost: \" + str(sum(lossesList)))\n",
    "        \n",
    "\n",
    "    next_state = stateToTorch(next_state)\n",
    "\n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "\n",
    "    _, next_value = model(next_state)\n",
    "    if (len(test_rewards) >= 800):\n",
    "        print(\"Are we alive: \" + str(not dead) + \", Game ended in a tie, total frames played: \" + str(frame_idx))\n",
    "    else:\n",
    "        print(\"Are we alive: \" + str(not dead) + \", Game ended after \" + str(len(masks)) + \" rounds, total frames played: \" + str(frame_idx))\n",
    "        \n",
    "    returns = compute_gae(next_value, rewards, masks, values)\n",
    "\n",
    "\n",
    "\n",
    "    returns   = torch.cat(returns).detach()\n",
    "    existingAggregate =varupdate(existingAggregate,sum(RNDrewards))\n",
    "\n",
    "    log_probs = torch.cat(log_probs).detach()\n",
    "    values    = torch.cat(values).detach()\n",
    "    states    = torch.cat(states)\n",
    "    actions   = torch.cat(actions)\n",
    "\n",
    "    returns = returns.view(-1, 1)\n",
    "    log_probs = log_probs.view(-1, num_outputs)\n",
    "    values = values.view(-1, 1)\n",
    "    states = states.view(-1, 7, 9, 9)\n",
    "    actions = actions.view(-1, num_outputs)\n",
    "    advantage = returns - values\n",
    "\n",
    "\n",
    "    targets = RandomNN(states)\n",
    "    predicts=PredictorNN(states)\n",
    "\n",
    "    lossPredict = loss(targets,predicts)\n",
    "\n",
    "    predictorOptim.zero_grad()\n",
    "    lossPredict.backward()\n",
    "    predictorOptim.step()\n",
    "\n",
    "\n",
    "    ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
